{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_3ozOcnYcNFW"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path, PosixPath\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.multinomial import Multinomial\n",
    "\n",
    "%qtconsole\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Siv21ARccNFY"
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 1: PyTorch Dataset</h2>\n",
    "\n",
    "Using PyTorch, data loading and preprocessing is done in so called [`Dataset`](https://pytorch.org/docs/stable/data.html) classes. The rules are:\n",
    "- The class has to inherit from PyTorch's Dataset class (`from torch.utils.data import Dataset`)\n",
    "- The class has to return the total number of samples in the `__len__(self)` method\n",
    "- The class has to return sample `idx` in the method `__getitem__(self, idx: int)`\n",
    "\n",
    "You can add any other functionality required to this class that is needed for e.g. loading your data into memory and preprocessing your data, but these three points are a must. If you follow these requirements, however, you can make use of PyTorchs `DataLoader` class, which automatically combines random samples into minibatches and out-sources data loading/preprocessing to parallel threads and does not block the model training.\n",
    "\n",
    "Here is a quick introduction to Datasets and DataLoader that might be useful to study: [Link](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
    "\n",
    "Regarding text data: We can't simply feed words or characters into our model, but have to _digitize_ them first. Do some internet/literature research yourself, how characters are usually encoded for model training and implement such an encoding into your Dataset class.\n",
    "\n",
    "Hint: You might want to have class methods that do the back-and-forth transformation. That is, given the model output, determine which character was predicted, and given any character, return the encoding. For the text generation later, also a function that returns the encoding for each letter in a sequence might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ss4wiSEgcNFb"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class TextDS(Dataset):\n",
    "    def __init__(self, text_file: PosixPath, seq_length: int, batch_size: int, vocabs= None):\n",
    "        self.vocab_input = vocabs\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.input_txt = open(text_file, encoding=\"utf-8\").readlines()\n",
    "        self.input_txt = ' '.join(self.input_txt)\n",
    "        self.txt_data = self.cleanse()\n",
    "\n",
    "        self.torch_sequences, self.vocabs, self.idx2char, self.char2idx = self.chars(seq_length)\n",
    "        self.torch_sequences = self.yield_sequence_split()\n",
    "        self.datagen = self.get_datagenerator()\n",
    "        \n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_torch)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.torch_sequences[idx]\n",
    "    \n",
    "    def cleanse(self):\n",
    "        \n",
    "        txt_data = self.input_txt.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').lower()\n",
    "        \n",
    "        charset = ['”', '“', '’', '‘', '\"', '$', '@', '[', ']', \"\" , '/' ,'0','1','2','3','4','5','6','7','8','9']\n",
    "        for i in charset:\n",
    "            txt_data = txt_data.replace(i , '')  \n",
    "                    \n",
    "        return txt_data\n",
    "    \n",
    "    def chars(self, seq_length):\n",
    "        if self.vocab_input != None:\n",
    "            vocab = self.vocab_input\n",
    "        else:\n",
    "            vocab = sorted(set(self.txt_data))\n",
    "        \n",
    "        char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "        idx2char = np.array(vocab)\n",
    "\n",
    "        text_as_int = np.array([char2idx[c] for c in self.txt_data])\n",
    "        \n",
    "        print('{} ---- characters mapped to int ---- > {}'.format(repr(self.txt_data[:13]), text_as_int[:13]))\n",
    "        print('Vocab size: ' + str(len(vocab)))\n",
    "        \n",
    "        length = seq_length\n",
    "        sequences = list()\n",
    "        for i in range(length, len(text_as_int)):\n",
    "            # select sequence of tokens\n",
    "            seq = text_as_int[i-length:i+1]\n",
    "            # store\n",
    "            sequences.append(seq)\n",
    "            \n",
    "                \n",
    "        sequences = torch.tensor(sequences).to(device)\n",
    "        \n",
    "        return sequences, vocab, idx2char, char2idx\n",
    "    \n",
    "    def yield_sequence_split(self):\n",
    "        \n",
    "        l = []\n",
    "        \n",
    "        for seq in self.torch_sequences:\n",
    "            input_example, target_example = self.split_input_target(seq)\n",
    "            l.append([input_example, target_example[-1]])\n",
    "        \n",
    "        return l     \n",
    "        \n",
    "    def get_datagenerator(self):\n",
    "        \n",
    "        datagen = torch.utils.data.DataLoader(self.torch_sequences, batch_size=self.batch_size)\n",
    "        return datagen\n",
    "\n",
    "    def split_input_target(self, chunk):\n",
    "        input_text = chunk[:-1]\n",
    "        target_text = chunk[1:]\n",
    "        return input_text, target_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "R5oJFaTncNFh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUuv-1gwcNFj"
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 2: PyTorch Model</h2>\n",
    "\n",
    "In the next step, you have to implement the LSTM as a PyTorch Module. Although the PyTorch library does already contain an LSTM implementation, you are asked to implement an LSTM in PyTorch from scratch. This is useful to understand better how PyTorch works and also to show you the differences compared to NumPy, when you want to implement your own custom model.\n",
    "\n",
    "Implementing a model in PyTorch has to follow these rules:\n",
    "- Your model class has to inherit from the `nn.Module` class (`import torch.nn as nn`)\n",
    "- You have to implement at least the `__init__()` and `forward()` method.\n",
    "    - The `__init__()` method is where you usually allocate the required network parameters (stored as class attributes).\n",
    "    - The `forward()` method implements the entire forward-pass through your model, from inputs to model predictions.\n",
    "    \n",
    "Beside that you can add any additional method. \n",
    "\n",
    "As stated above, you don't have to implement the gradient computation (backward pass), which is done for you automatically with autograd. However, for this to work, you can only use PyTorch functions in your forward pass ( + standard math operators like e.g. [+, -, *, /]). But coming from NumPy, this requires little changes, since most NumPy functions are implemented with the same name in PyTorch.\n",
    "\n",
    "**Note**: You are not allowed to use the PyTorch LSTM layer but you can use other pre-implemented PyTorch layers, like `nn.Linear` if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xfWHVOfScNFl"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size, output_size, n_of_classes, seq_length, LR=0.001, momentum= 0.9):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_of_classes = n_of_classes\n",
    "        \n",
    "        self.z = nn.Linear(input_size, hidden_size)\n",
    "        self.z_rec = nn.Linear(hidden_size,hidden_size) \n",
    "        self.z_tanh = nn.Tanh()\n",
    "        \n",
    "        self.i = nn.Linear(input_size, hidden_size)\n",
    "        self.i_rec = nn.Linear(hidden_size, hidden_size)\n",
    "        self.i_sigmoid = nn.Sigmoid()\n",
    "                \n",
    "        self.o = nn.Linear(input_size, hidden_size)\n",
    "        self.o_rec = nn.Linear(hidden_size, hidden_size)\n",
    "        self.o_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.c_tanh = nn.Tanh()\n",
    "        \n",
    "        self.h0 = torch.zeros((hidden_size)).type(torch.float)\n",
    "        self.c0 = torch.zeros((hidden_size)).type(torch.float)\n",
    "        \n",
    "        self.V = nn.Linear(hidden_size, n_of_classes)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "        #self.optimizer = torch.optim.SGD(self.parameters(), lr=LR, momentum=momentum)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "        self.current_loss = []\n",
    "        \n",
    "        \n",
    "    def forward(self, batch) -> torch.Tensor:\n",
    "         \n",
    "        batch_size = len(batch)\n",
    "        seq_len = batch.shape[1]\n",
    "        \n",
    "        self.h0 = torch.zeros((batch_size, hidden_size)).type(torch.float).to(device)\n",
    "        self.c0 = torch.zeros((batch_size, hidden_size)).type(torch.float).to(device)\n",
    "        \n",
    "\n",
    "        for i in range(seq_len):\n",
    "\n",
    "            x= batch[:,i,:]\n",
    "            \n",
    "            y_z = self.z_tanh(self.z(x) + self.z_rec(self.h0))\n",
    "            y_i = self.i_sigmoid(self.i(x) + self.i_rec(self.h0))\n",
    "\n",
    "            c = self.c0 + y_z * y_i\n",
    "            self.c0 = c.detach()\n",
    "\n",
    "            y_c = self.c_tanh(c)\n",
    "            y_o = self.o_sigmoid(self.o(x) + self.o_rec(self.h0))\n",
    "\n",
    "            y = y_c * y_o\n",
    "            self.h0 = y.detach()  # rec. y; detach grad attribute\n",
    "        \n",
    "        y = self.V(y)  # puts y into the right shape for cross entropy\n",
    "             \n",
    "        return y\n",
    "    \n",
    "    def print_grads(self):\n",
    "        print(sum(self.z.weight.grad))\n",
    "    \n",
    "    def reset_loss(self):\n",
    "        self.current_loss = []\n",
    "        \n",
    "    def backward(self, y_hat, y):\n",
    "        loss_ = self.loss(y_hat, y)\n",
    "        loss_.backward(retain_graph=True)\n",
    "  \n",
    "        self.current_loss.append(loss_.detach())\n",
    "        \n",
    "    def update(self):\n",
    "        self.optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFP_DawbcNF2"
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 3: Model training and validation</h2>\n",
    "\n",
    "Here you should implement everything you need for training your model and validate it during the training process. From our previous assignments and the PyTorch tutorial linked above, you should be able to implement everything required.\n",
    "\n",
    "This time we leave you freedom to find your own way of implementing this. If you are struggeling with getting to start, you could fall back to the structure of the `Learner` class you had to implement in Assignment 2.\n",
    "\n",
    "Hints:\n",
    "- You should use the text in the `*_train.txt` file to train your model and in the `*_val.txt` file to validate your model. That is, you don't want to perform any weight update with the content of the validation file, but only check your model performance from time to time (e.g. after each epoch) on indepent data.\n",
    "- You should log your training loss and validation loss \n",
    "- Look for an appropriate loss function for multi-class classification\n",
    "- You can use any optimizer from the `torch.optim` module, such as `torch.optim.SGD` or `torch.optim.Adam`.\n",
    "- You have to decide on the sequence length you want to feed into the model, for predicting the next character. Long sequences mean longer run-times but also more context for the model to learn from. A length of 100 might be a good starting point but please do your own experiments with this hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyxCnUtPcNGA",
    "outputId": "d94b7e8e-2fd4-4e2a-98c1-fa9d538d72a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'speech    ...' ---- characters mapped to int ---- > [33 30 19 19 17 22  0  0  0  0  9  9  9]\n",
      "Vocab size: 45\n",
      "'good evening.' ---- characters mapped to int ---- > [21 29 29 18  0 19 36 19 28 23 28 21  9]\n",
      "Vocab size: 45\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "seq_length_ = 120\n",
    "batch_size_ = 64\n",
    "lr = 0.001\n",
    "hidden_size = 1024\n",
    "epochs = 10\n",
    "\n",
    "data = TextDS('sample_data/trump_train.txt', seq_length_, batch_size_, None)\n",
    "data_validation = TextDS('sample_data/trump_val.txt', seq_length_, batch_size_, data.vocabs) #use vocab idx of train\n",
    "\n",
    "datagen = data.datagen\n",
    "datagen_validation = data_validation.datagen\n",
    "\n",
    "n_of_classes = len(data.vocabs)\n",
    "sequence_length = data.seq_length\n",
    "\n",
    "input_size = n_of_classes\n",
    "hidden_size = hidden_size\n",
    "output_size = n_of_classes\n",
    "\n",
    "\n",
    "m = Model(input_size, hidden_size, output_size, n_of_classes, sequence_length, lr)\n",
    "m.to(device) #enable cuda\n",
    "\n",
    "loss_list = list()\n",
    "valid_loss_list = []\n",
    "accuracy_list = list()\n",
    "correct = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HKeBdsahcNGB",
    "outputId": "5ce1a7a3-b6ba-4708-b83b-d0e6be2d8cce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Loss: 1.627, Accuracy: 0.514\n",
      "Step: 2000, Loss: 1.573, Accuracy: 0.534\n",
      "Step: 3000, Loss: 1.544, Accuracy: 0.541\n",
      "Step: 4000, Loss: 1.574, Accuracy: 0.531\n",
      "Step: 5000, Loss: 1.51, Accuracy: 0.548\n",
      "Step: 6000, Loss: 1.443, Accuracy: 0.565\n",
      "Step: 7000, Loss: 1.469, Accuracy: 0.56\n",
      "Step: 8000, Loss: 1.367, Accuracy: 0.587\n",
      "Step: 9000, Loss: 1.434, Accuracy: 0.569\n",
      "Step: 10000, Loss: 1.349, Accuracy: 0.591\n",
      "Step: 11000, Loss: 1.324, Accuracy: 0.598\n",
      "Step: 12000, Loss: 1.342, Accuracy: 0.594\n",
      "Step: 13000, Loss: 1.37, Accuracy: 0.585\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid. Loss:tensor(1.5363, device='cuda:0')\n",
      "Step: 1000, Loss: 1.392, Accuracy: 0.581\n",
      "Step: 2000, Loss: 1.281, Accuracy: 0.612\n",
      "Step: 3000, Loss: 1.268, Accuracy: 0.616\n",
      "Step: 4000, Loss: 1.313, Accuracy: 0.599\n",
      "Step: 5000, Loss: 1.271, Accuracy: 0.61\n",
      "Step: 6000, Loss: 1.227, Accuracy: 0.623\n",
      "Step: 7000, Loss: 1.262, Accuracy: 0.615\n",
      "Step: 8000, Loss: 1.175, Accuracy: 0.639\n",
      "Step: 9000, Loss: 1.25, Accuracy: 0.617\n",
      "Step: 10000, Loss: 1.186, Accuracy: 0.639\n",
      "Step: 11000, Loss: 1.175, Accuracy: 0.639\n",
      "Step: 12000, Loss: 1.195, Accuracy: 0.634\n",
      "Step: 13000, Loss: 1.221, Accuracy: 0.623\n",
      "\n",
      "Epoch 2\n",
      "Valid. Loss:tensor(1.4098, device='cuda:0')\n",
      "Step: 1000, Loss: 1.277, Accuracy: 0.609\n",
      "Step: 2000, Loss: 1.159, Accuracy: 0.643\n",
      "Step: 3000, Loss: 1.157, Accuracy: 0.644\n",
      "Step: 4000, Loss: 1.189, Accuracy: 0.632\n",
      "Step: 5000, Loss: 1.167, Accuracy: 0.638\n",
      "Step: 6000, Loss: 1.129, Accuracy: 0.649\n",
      "Step: 7000, Loss: 1.165, Accuracy: 0.638\n",
      "Step: 8000, Loss: 1.088, Accuracy: 0.661\n",
      "Step: 9000, Loss: 1.158, Accuracy: 0.642\n",
      "Step: 10000, Loss: 1.105, Accuracy: 0.658\n",
      "Step: 11000, Loss: 1.1, Accuracy: 0.658\n",
      "Step: 12000, Loss: 1.121, Accuracy: 0.652\n",
      "Step: 13000, Loss: 1.142, Accuracy: 0.645\n",
      "\n",
      "Epoch 3\n",
      "Valid. Loss:tensor(1.3555, device='cuda:0')\n",
      "Step: 1000, Loss: 1.208, Accuracy: 0.629\n",
      "Step: 2000, Loss: 1.093, Accuracy: 0.66\n",
      "Step: 3000, Loss: 1.095, Accuracy: 0.661\n",
      "Step: 4000, Loss: 1.111, Accuracy: 0.653\n",
      "Step: 5000, Loss: 1.105, Accuracy: 0.657\n",
      "Step: 6000, Loss: 1.07, Accuracy: 0.666\n",
      "Step: 7000, Loss: 1.103, Accuracy: 0.654\n",
      "Step: 8000, Loss: 1.035, Accuracy: 0.674\n",
      "Step: 9000, Loss: 1.1, Accuracy: 0.657\n",
      "Step: 10000, Loss: 1.054, Accuracy: 0.671\n",
      "Step: 11000, Loss: 1.051, Accuracy: 0.67\n",
      "Step: 12000, Loss: 1.072, Accuracy: 0.666\n",
      "Step: 13000, Loss: 1.09, Accuracy: 0.659\n",
      "\n",
      "Epoch 4\n",
      "Valid. Loss:tensor(1.3239, device='cuda:0')\n",
      "Step: 1000, Loss: 1.159, Accuracy: 0.641\n",
      "Step: 2000, Loss: 1.049, Accuracy: 0.672\n",
      "Step: 3000, Loss: 1.054, Accuracy: 0.67\n",
      "Step: 4000, Loss: 1.056, Accuracy: 0.668\n",
      "Step: 5000, Loss: 1.064, Accuracy: 0.666\n",
      "Step: 6000, Loss: 1.03, Accuracy: 0.677\n",
      "Step: 7000, Loss: 1.059, Accuracy: 0.666\n",
      "Step: 8000, Loss: 0.999, Accuracy: 0.683\n",
      "Step: 9000, Loss: 1.056, Accuracy: 0.668\n",
      "Step: 10000, Loss: 1.019, Accuracy: 0.681\n",
      "Step: 11000, Loss: 1.015, Accuracy: 0.679\n",
      "Step: 12000, Loss: 1.035, Accuracy: 0.676\n",
      "Step: 13000, Loss: 1.05, Accuracy: 0.671\n",
      "\n",
      "Epoch 5\n",
      "Valid. Loss:tensor(1.3044, device='cuda:0')\n",
      "Step: 1000, Loss: 1.121, Accuracy: 0.65\n",
      "Step: 2000, Loss: 1.016, Accuracy: 0.683\n",
      "Step: 3000, Loss: 1.019, Accuracy: 0.681\n",
      "Step: 4000, Loss: 1.011, Accuracy: 0.681\n",
      "Step: 5000, Loss: 1.031, Accuracy: 0.675\n",
      "Step: 6000, Loss: 0.999, Accuracy: 0.686\n",
      "Step: 7000, Loss: 1.024, Accuracy: 0.676\n",
      "Step: 8000, Loss: 0.971, Accuracy: 0.692\n",
      "Step: 9000, Loss: 1.021, Accuracy: 0.679\n",
      "Step: 10000, Loss: 0.991, Accuracy: 0.688\n",
      "Step: 11000, Loss: 0.987, Accuracy: 0.687\n",
      "Step: 12000, Loss: 1.005, Accuracy: 0.684\n",
      "Step: 13000, Loss: 1.019, Accuracy: 0.678\n",
      "\n",
      "Epoch 6\n",
      "Valid. Loss:tensor(1.2910, device='cuda:0')\n",
      "Step: 1000, Loss: 1.092, Accuracy: 0.658\n",
      "Step: 2000, Loss: 0.991, Accuracy: 0.689\n",
      "Step: 3000, Loss: 0.994, Accuracy: 0.688\n",
      "Step: 4000, Loss: 0.974, Accuracy: 0.691\n",
      "Step: 5000, Loss: 1.006, Accuracy: 0.683\n",
      "Step: 6000, Loss: 0.973, Accuracy: 0.691\n",
      "Step: 7000, Loss: 0.998, Accuracy: 0.685\n",
      "Step: 8000, Loss: 0.949, Accuracy: 0.697\n",
      "Step: 9000, Loss: 0.995, Accuracy: 0.686\n",
      "Step: 10000, Loss: 0.971, Accuracy: 0.694\n",
      "Step: 11000, Loss: 0.964, Accuracy: 0.694\n",
      "Step: 12000, Loss: 0.982, Accuracy: 0.69\n",
      "Step: 13000, Loss: 0.993, Accuracy: 0.685\n",
      "\n",
      "Epoch 7\n",
      "Valid. Loss:tensor(1.2885, device='cuda:0')\n",
      "Step: 1000, Loss: 1.068, Accuracy: 0.666\n",
      "Step: 2000, Loss: 0.969, Accuracy: 0.695\n",
      "Step: 3000, Loss: 0.971, Accuracy: 0.694\n",
      "Step: 4000, Loss: 0.943, Accuracy: 0.699\n",
      "Step: 5000, Loss: 0.985, Accuracy: 0.689\n",
      "Step: 6000, Loss: 0.952, Accuracy: 0.698\n",
      "Step: 7000, Loss: 0.976, Accuracy: 0.689\n",
      "Step: 8000, Loss: 0.929, Accuracy: 0.703\n",
      "Step: 9000, Loss: 0.972, Accuracy: 0.693\n",
      "Step: 10000, Loss: 0.952, Accuracy: 0.699\n",
      "Step: 11000, Loss: 0.946, Accuracy: 0.699\n",
      "Step: 12000, Loss: 0.964, Accuracy: 0.695\n",
      "Step: 13000, Loss: 0.973, Accuracy: 0.691\n",
      "\n",
      "Epoch 8\n",
      "Valid. Loss:tensor(1.2910, device='cuda:0')\n",
      "Step: 1000, Loss: 1.048, Accuracy: 0.671\n",
      "Step: 2000, Loss: 0.951, Accuracy: 0.699\n",
      "Step: 3000, Loss: 0.953, Accuracy: 0.698\n",
      "Step: 4000, Loss: 0.919, Accuracy: 0.705\n",
      "Step: 5000, Loss: 0.968, Accuracy: 0.694\n",
      "Step: 6000, Loss: 0.937, Accuracy: 0.702\n",
      "Step: 7000, Loss: 0.96, Accuracy: 0.694\n",
      "Step: 8000, Loss: 0.914, Accuracy: 0.708\n",
      "Step: 9000, Loss: 0.956, Accuracy: 0.697\n",
      "Step: 10000, Loss: 0.937, Accuracy: 0.703\n",
      "Step: 11000, Loss: 0.932, Accuracy: 0.702\n",
      "Step: 12000, Loss: 0.949, Accuracy: 0.699\n",
      "Step: 13000, Loss: 0.956, Accuracy: 0.695\n",
      "\n",
      "Epoch 9\n",
      "Valid. Loss:tensor(1.2993, device='cuda:0')\n",
      "Step: 1000, Loss: 1.032, Accuracy: 0.674\n",
      "Step: 2000, Loss: 0.937, Accuracy: 0.703\n",
      "Step: 3000, Loss: 0.941, Accuracy: 0.702\n",
      "Step: 4000, Loss: 0.902, Accuracy: 0.71\n",
      "Step: 5000, Loss: 0.954, Accuracy: 0.696\n",
      "Step: 6000, Loss: 0.922, Accuracy: 0.707\n",
      "Step: 7000, Loss: 0.945, Accuracy: 0.699\n",
      "Step: 8000, Loss: 0.902, Accuracy: 0.711\n",
      "Step: 9000, Loss: 0.943, Accuracy: 0.701\n",
      "Step: 10000, Loss: 0.925, Accuracy: 0.705\n",
      "Step: 11000, Loss: 0.923, Accuracy: 0.705\n",
      "Step: 12000, Loss: 0.936, Accuracy: 0.704\n",
      "Step: 13000, Loss: 0.943, Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('')\n",
    "    print('Epoch ' + str(epoch))\n",
    "    \n",
    "    if epoch != 0:\n",
    "        loss_list.append((sum(m.current_loss) / len(m.current_loss)))\n",
    "        m.reset_loss()\n",
    "        \n",
    "        accuracy_list.append(sum(correct) / len(correct))\n",
    "        correct = []\n",
    "        \n",
    "    # *******  Validation ************#\n",
    "        valid_loss = []\n",
    "        with torch.no_grad():\n",
    "            for x_valid, target_valid in datagen_validation:\n",
    "                x_valid = nn.functional.one_hot(x_valid.type(torch.long), n_of_classes).type(torch.float)\n",
    "                y_hat_valid = m.forward(x_valid)  \n",
    "                y_valid = torch.tensor(target_valid.type(torch.long))\n",
    "                valid_loss.append(m.loss(y_hat_valid,y_valid))\n",
    "\n",
    "        print('Valid. Loss:' + str(sum(valid_loss) / len(valid_loss)))\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_loss = []\n",
    "        \n",
    "\n",
    "    # ********** Training *******************#\n",
    "    for step_idx, (x,target) in enumerate(datagen):\n",
    "               \n",
    "        m.optimizer.zero_grad()\n",
    "\n",
    "        x = nn.functional.one_hot(x.type(torch.long), n_of_classes).type(torch.float)\n",
    "        y_hat = m.forward(x)  \n",
    "        y = torch.tensor(target.type(torch.long))\n",
    "        \n",
    "        y_for_acc = torch.argmax(torch.nn.functional.softmax(y_hat.detach()).data, axis=1)\n",
    "        correct.append(torch.sum((y_for_acc == y).float()) / len(x))\n",
    "                \n",
    "        \n",
    "        m.backward(y_hat,y)\n",
    "        m.update()\n",
    "        \n",
    "        if step_idx % 1000 == 0 and step_idx != 0:\n",
    "            tmp = m.current_loss[-1000:]\n",
    "            print('Step: ' + str(step_idx)+ ', ' +'Loss: ' + str(round(float(sum(tmp) / len(tmp)),3)) + ', ' + 'Accuracy: ' + str(round(float(sum(correct[-1000:]) / 1000),3)) )  \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfUUml67cNGC"
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 4: Visualize the training progress</h2>\n",
    "\n",
    "Create visualization(s) showing the training and validation loss over the course of the model training and write a short figure caption to explain your plot(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "OTmyNV_KcNGD",
    "outputId": "440afbb2-74d5-4a0b-be4b-381aaa49d571"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"loss curves\")\n",
    "\n",
    "loss_train = np.array(loss_list)\n",
    "loss_valid = np.mean(np.array(valid_loss_list), axis=1)\n",
    "\n",
    "\n",
    "loss_curve, = plt.plot(loss_train, label=f'train (acc: {100 * float(accuracy_list[-1]):2.2f}%)')\n",
    "\n",
    "plt.plot(loss_valid, linestyle='--', color=loss_curve.get_color(), label=f'valid')\n",
    "plt.ylim(0, 5)  # you can change the upper limit if you want\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross Entropy Error')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsduUKP6cNGF"
   },
   "source": [
    "Your figure caption here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_wzGMtzcNGH"
   },
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 5: Generate text</h2>\n",
    "\n",
    "Now that you have a trained CharRNN, go on and generate some text. Therefore, start of with some initial text string you provide the model as the initial text and then predict character by character, always adding the previous predicted character to the input sequence for the next character.\n",
    "\n",
    "**Bonus point:**\n",
    "In practice, for this kind of task, you do not want to pick always the character with the highest probability, but instead pick randomly one of the top k classes/characters (with k e.g. 3 or 5). Adapt your text generation code to add this random sampling from the top k predictions to get a bonus point for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yASF7MsTcNGI",
    "outputId": "319827ed-9ed6-46ea-f6e3-43856d332de7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mean I've made so much money fighting against the Chinese a\n",
      "I mean I've made so much money fighting against the Chinese an\n",
      "I mean I've made so much money fighting against the Chinese and\n",
      "I mean I've made so much money fighting against the Chinese and \n",
      "I mean I've made so much money fighting against the Chinese and t\n",
      "I mean I've made so much money fighting against the Chinese and th\n",
      "I mean I've made so much money fighting against the Chinese and the\n",
      "I mean I've made so much money fighting against the Chinese and the \n",
      "I mean I've made so much money fighting against the Chinese and the t\n",
      "I mean I've made so much money fighting against the Chinese and the tr\n",
      "I mean I've made so much money fighting against the Chinese and the tru\n",
      "I mean I've made so much money fighting against the Chinese and the trum\n",
      "I mean I've made so much money fighting against the Chinese and the trump\n",
      "I mean I've made so much money fighting against the Chinese and the trump \n",
      "I mean I've made so much money fighting against the Chinese and the trump a\n",
      "I mean I've made so much money fighting against the Chinese and the trump an\n",
      "I mean I've made so much money fighting against the Chinese and the trump and\n",
      "I mean I've made so much money fighting against the Chinese and the trump and,\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, i\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it w\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it wi\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it wil\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will b\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be a\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be am\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be ame\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be amer\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be ameri\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be americ\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be america\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american s\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american st\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american ste\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american stee\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel t\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel th\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel tha\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that w\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we h\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we ha\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we hav\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have.\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. t\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. th\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. the\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they a\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they ar\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are g\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are go\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are goi\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are goin\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going t\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to b\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to be\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to bel\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to beli\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to belie\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believ\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe i\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in t\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in th\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the w\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the wo\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the wor\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the worl\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the world\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the world \n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the world t\n",
      "I mean I've made so much money fighting against the Chinese and the trump and, it will be american steel that we have. they are going to believe in the world tr\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_sentence = \"I mean I've made so much money fighting against the Chinese \"\n",
    "\n",
    "top_k = 2\n",
    "n_of_chars = 100\n",
    "\n",
    "def softmax_pred(X):\n",
    "    expo = torch.exp(X)\n",
    "    expo_sum = torch.sum(torch.exp(X))\n",
    "    soft = expo/expo_sum\n",
    "    pred = torch.argmax(soft)\n",
    "    if top_k > 1 and pred < 0.7:                 # < 0.7 condition for only doing randomization only in unsure cases for text quality\n",
    "        vals, topk_idx = torch.topk(soft, top_k)\n",
    "        idx = torch.randint(0, top_k, (1,))\n",
    "        pred = topk_idx[0][idx]\n",
    "    return pred\n",
    "\n",
    "for i in range(n_of_chars):\n",
    "\n",
    "    text_as_int = np.array([data.char2idx[c] for c in start_sentence.lower()])\n",
    "    text_one_hot = nn.functional.one_hot(torch.tensor(text_as_int).type(torch.long), n_of_classes).type(torch.float)\n",
    "    \n",
    "    input_tensor = text_one_hot.reshape((1, text_one_hot.shape[0], text_one_hot.shape[1])).to(device)\n",
    "\n",
    "    output = m.forward(input_tensor).detach()\n",
    "\n",
    "    pred = softmax_pred(output.to(device))\n",
    "\n",
    "    char = data.idx2char[pred]\n",
    "    \n",
    "    start_sentence += char\n",
    "\n",
    "    print(start_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ix7EsqO1Ek0a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
